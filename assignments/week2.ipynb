{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions: A Simple Example with Matrix Factorisation.\n",
    "\n",
    "### 6th October 2015 Neil D. Lawrence\n",
    "\n",
    "In [last week's](./week1.ipynb) class we saw how we could load in a data set to pandas and use it for some simple data processing. We computed variaous probabilities on the data and I encouraged you to think about what sort of probabilities you need for prediction. This week we are going to take a slightly different tack. \n",
    "\n",
    "Broadly speaking there are two dominating approaches to machine learning problems. We started to consider the first approach last week: constructing models based on defining the relationship between variables using probabilities. This week we will consider the second approach: which involves defining an *objective function* and optimizing it. \n",
    "\n",
    "What do we mean by an objective function? An objective function could be an *error function* a *cost function* or a *benefit* function. In evolutionary computing they are called *fitness* functions. But the idea is always the same. We write down a mathematical equation which is then optimized to do the learning. The equation should be a function of the *data* and our model *parameters*. We have a choice when optimizing, either minimize or maximize. To avoid confusion, in the optimization field, we always choose to minimize the function. If we have function that we would like to maximize, we simply choose to minimize the negative of that function. \n",
    "\n",
    "So for this lab session, we are going to ignore probabilities, but don't worry, they will return! \n",
    "\n",
    "This week we are going to try and build a simple movie recommender system using an objective function. To do this, the first thing I'd like you to do is to install some software we've written for sharing information across google documents.\n",
    "\n",
    "## Open Data Science Software\n",
    "\n",
    "In Sheffield we have written a suite of software tools for 'Open Data Science'. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques. For some background on open data science you can read [this blog post](http://inverseprobability.com/2014/07/01/open-data-science/). The first thing we will do this week is to download that suite of software. \n",
    "\n",
    "The software can be installed using\n",
    "\n",
    "```python\n",
    "pip --pre install pods\n",
    "```\n",
    "\n",
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "### Download the Movie Body Counts Data\n",
    "\n",
    "Now that we have the Open Data Science Software installed we can download the movie body counts data. When you run the commands below for the first time on a computer you will be asked to accept the download and acknowledge the source of the data. When you use a data set that someone has prepared you should always reference the data source to acknowledge the work that's been placed in. The body counts data contains the work of the [researchers at NJIT](http://www.theswarmlab.com/r-vs-python-round-2-22/) and also the work of the [IMDb website](http://www.imdb.com) and [www.MovieBodyCounts.com](http://www.MovieBodyCounts.com). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring resource: movie_body_count\n",
      "\n",
      "Details of data: \n",
      "Data scraped from www.MovieBodyCounts.com and www.imdb.com using scripts provided on a github repository (in both Python and R) at https://github.com/morpionZ/R-vs-Python/tree/master/Deadliest%20movies%20scrape/code. This script pulls down the scraped data.\n",
      "\n",
      "Please cite:\n",
      "Simon Garnier and Randy Olson, Blog Post: R vs Python Round 2, February 2nd 2014 (http://www.theswarmlab.com/r-vs-python-round-2-22/)\n",
      "\n",
      "After downloading the data will take up 536272 bytes of space.\n",
      "\n",
      "Data will be stored in /home/nathan/ods_data_cache/movie_body_count.\n",
      "\n",
      "Do you wish to proceed with the download? [yes/no]\n",
      "yes\n",
      "Downloading  https://github.com/sjmgarnier/R-vs-Python/raw/master/Deadliest%20movies%20scrape/code/film-death-counts-Python.csv -> /home/nathan/ods_data_cache/movie_body_count/film-death-counts-Python.csv\n",
      "[==============================]   0.366/0.366MB\n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "d = pods.datasets.movie_body_count()\n",
    "movies = d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 1\n",
    "\n",
    "Data ethics. If you find data available on the internet, can you simply use it without consequence? If you are given data by a fellow researcher can you publish that data on line? \n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "You should always check with sources before publishing as well as check any licensing constraints. Also, the origin of the data or creative works should always be referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "A recommender system aims to make suggestions for items (films, books, other commercial products) given what it knows about users' tastes. The recommendation engine needs to represent the *taste* of all the users and the *characteristics* of each object. \n",
    "\n",
    "A common way for organizing objects is to place related objects spatially close together. For example in a library we try and put books that are on related topics near to each other on the shelves. One system for doing this is known as [Dewey Decimal Classification](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification). In the Dewey Decimal Classification system (which dates from 1876) each subject is given a number (in fact it's a decimal number). For example, the field of Natural Sciences and Mathematics is given numbers which start with 500. Subjects based on Computer Science are given numbers which start 004 and works on the 'mathematical principles' of Computer science are given the series 004.0151 (which we might store as 4.0151 on a Computer). Whilst it's a classification system, the books in the library are typically laid out in the same order as the numbers, so we might expect that neighbouring numbers represent books that are related in subject. That seems to be exactly what we want when also representing films. Could we somehow represent each film's subject according to a number? In a similar way we could then imagine representing users with a list of numbers that represent things that each user is interested in.\n",
    "\n",
    "Actually a one dimensional representation of a subject can be very awkward. To see this, let's have a look at the Dewey Decimal Classification numbers for the 900s, which is listed as 'History and Geography'. We will focus on subjects in the 940s which can be found in this list from [Nova Southeastern University](http://www.nova.edu/library/help/misc/lc_dewey/dewey900.html#40). Whilst the ordering for places is somewhat sensible, it is also rather arbitrary. In the 940s we have Europe listed from 940-949, Asia listed from 950-959 and Africa listed from 960-969. Whilst it's true that Asia borders Europe, Africa is also very close, and the history of the Roman Empire spreads into [Carthage](http://en.wikipedia.org/wiki/Carthage) and later on Egypt. This image from Wikipedia shows a map of the Cathaginian Empire which fell after fighting with Rome. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/Carthaginianempire.PNG\", width=500>\n",
    "\n",
    "We now need to make a decision about whether Roman Histories are European or African, ideally we'd like them to be somewhere between the two, but we can't place them there in the Dewey Decimal system because between Europe and Africa is Asia, which has less to do with the Roman Empire than either Europe or Africa. Of course the fact that we've used a map provides a clue as to what to do next. Libraries are actually laid out on floors, so what if we were to use the spatial lay out to organise the sujbects of the books in two dimensions. Books on Geography could be laid out according to where in the world they are referring to. \n",
    "\n",
    "Such complexities are very hard to encapsulate in one number, but inspired by the map examples we can start considering how we might lay out films in two dimensions. Similarly, we can consider laying out a map of people's interests. If the two maps correspond to one another, the map of people could reflect where they might want to live in 'subject space'. We can think of representing people's tastes as where they might best like to sit in the library to access easily the books they are most interested in.\n",
    "\n",
    "\n",
    "## Inner Products for Representing Similarity\n",
    "\n",
    "Ideas like the above are good for gaining intuitions about what we might want, but the one of the skills of data science is representing those ideas mathematically. Mathematical abstraction of a problem is one of the key ways in which we've been able to progress as a society. Understanding planetary motions, as well as those of the smallest molecule (to quote Laplace's [Philosophical Essay on Probabilities](http://books.google.co.uk/books?id=1YQPAAAAQAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)) needed to be done mathematically. The right mathematical model in machine learning can be slightly more elusive, because constructing it is a two stage process. \n",
    "\n",
    "1. We have to determine the right intuition for the system we want to represent. Notions such as 'subject' and 'interest' are not mathematically well defined, and even when we create a new interpretation of what they might mean, each interpretation may have its own weaknesses. \n",
    "\n",
    "2. Once we have our interpretation we can attempt to mathematically formalize it. In our library interpretation, that's what we need to do next. \n",
    "\n",
    "### The Library on an Infinite Plane\n",
    "\n",
    "Let's imagine a library which stores all the items  we are interested in, not just books, but films and shopping items too. Such a library is likely to be very large, so we'll create it on an infinite two dimensional plane. This means we can use all the real numbers to represent the location of each item on the plane. For a two dimensional plane, we need to store the locations in a vector of numbers: we can decide that the $j$th item's location in the library is given by \n",
    "$$\n",
    "\\mathbf{v}_j = \\begin{bmatrix} v_{j,1} \\\\ v_{j,2}\\end{bmatrix},\n",
    "$$\n",
    "where $v_{j,1}$ represents the $j$th item's location in the East-West direction (or the $x$-axis) and $v_{j,2}$ represents the $j$th item's location in the North-South direction (or the $y$-axis). Now we need to specify the location where each user sits so that all the items that interest them are nearby: we can also represent the $i$th user's location with a vector \n",
    "$$\n",
    "\\mathbf{u}_i = \\begin{bmatrix} u_{i,1} \\\\ u_{i,2}\\end{bmatrix}.\n",
    "$$\n",
    "Finally, we need some way of recording a given user's affinity for a given item. This affinity might be the rating that the user gives the film. We can use $y_{i,j}$ to represent user $i$'s affinity for item $j$. \n",
    "\n",
    "For our film example we might imagine wanting to order films in a few ways. We could imagine organising films in the North-South direction as to how romantic they are. We could place the more romantic films further North and the less romantic films further South. For the East-West direction we could imagine ordering them according to how historic they are: we can imagine placing science fiction films to the East and historical drama to the West. In this case, fans of historical romances would be based in the North-West location, whilst fans of Science Fiction Action films might be located in the South-East (if we assume that 'Action' is the opposite of 'Romance', which is not necessarily the case). How do we lay out all these films? Have we got the right axes? In machine learning the answer is to 'let the data speak'. Use the data to try and obtain such a lay out. To do this we first need to obtain the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We are using a functionality of the Open Data Science software library to obtain the data. This functionality involves some prewritten code which distributes to each of you a google spreadsheet where you can rate movies that you've seen. For completeness the code follows. Try and read and understand the code, but don't run it! It has already been run centrally by me. \n",
    "\n",
    "``` python\n",
    "\n",
    "import pods\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "user_data = pd.DataFrame(index=movies.index, columns=['title', 'year', 'rating', 'prediction'])\n",
    "user_data['title']=movies.Film\n",
    "user_data['year']=movies.Year\n",
    "\n",
    "accumulator=pods.lab.distributor(spreadsheet_title='COM4509/6509 Movie Ratings:', user_sep='\\t')\n",
    "# function to apply to data before giving it to user \n",
    "# Select 50 movies at random and order them according to year.\n",
    "max_movies = 50\n",
    "function = lambda x: x.loc[np.random.permutation(x.index)[:max_movies]].sort(columns='year')\n",
    "accumulator.write(data_frame=user_data, comment='Film Ratings', \n",
    "                  function=function)\n",
    "accumulator.write_comment('Rate Movie Here (score 1-5)', row=1, column=4)\n",
    "accumulator.share(share_type='writer', send_notifications=True)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In your Google docs account you should be able to find a spreadsheet called 'COM4509/6509 Movie Ratings: Your Name'. In the spreadsheet You should find a number of movies listed according to year. In the column titled 'rating' please place your rating using a score of 1-5 for *any* of the movies you've seen from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have placed your ratings we can download the data from your spreadsheets to a central file where the ratings of the whole class can be stored. We will build an algorithm on these ratings and use them to make predictions for the rest of the class. Firstly, here's the code for reading the ratings from each of the spreadsheets.\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "accumulator = pods.lab.distributor(user_sep='\\t')\n",
    "data = accumulator.read(usecols=['rating'], dtype={'index':int, 'rating':np.float64}, header=2)\n",
    "\n",
    "for user in data:\n",
    "    if data[user].rating.count()>0: # only add user if they rated something\n",
    "        # add a new field to movies with that user's ratings.\n",
    "        movies[user] = data[user]['rating']\n",
    "\n",
    "# Store the csv on disk where it will be shared through dropbox.\n",
    "movies.to_csv(os.path.join(pods.lab.class_dir,'movies.csv'), index_label='index')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our data structure into a form that is appropriate for processing. We will convert the `movies` object into a data base which contains the movie, the user and the score using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  https://dl.dropboxusercontent.com/u/4347554/mlai_movies.csv -> ./class_movie/movies.csv\n",
      "[==============================]   0.380/0.380MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# uncomment the line below if you are doing this task by self study.\n",
    "pods.util.download_url('https://dl.dropboxusercontent.com/u/4347554/mlai_movies.csv', store_directory = 'class_movie', save_name='movies.csv')\n",
    "#pods.util.download_url('https://www.dropbox.com/s/s6gqvp9b383b59y/movies.csv?dl=0&raw=1', store_directory = 'class_movie', save_name='movies.csv')\n",
    "movies = pd.read_csv(os.path.join('class_movie', 'movies.csv'),encoding='latin-1').set_index('index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Body_Count</th>\n",
       "      <th>Length_Minutes</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Bob</th>\n",
       "      <th>Arthur</th>\n",
       "      <th>George</th>\n",
       "      <th>Peter</th>\n",
       "      <th>John</th>\n",
       "      <th>Terry</th>\n",
       "      <th>...</th>\n",
       "      <th>Cedric</th>\n",
       "      <th>David</th>\n",
       "      <th>Nathaniel</th>\n",
       "      <th>Helen</th>\n",
       "      <th>Elaine</th>\n",
       "      <th>Toni</th>\n",
       "      <th>Joanne</th>\n",
       "      <th>Elizabeth</th>\n",
       "      <th>Mary</th>\n",
       "      <th>Adeline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>421.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1996.491686</td>\n",
       "      <td>53.287411</td>\n",
       "      <td>115.427553</td>\n",
       "      <td>6.882898</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>3.470588</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.80000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.095238</td>\n",
       "      <td>4</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.409091</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.913210</td>\n",
       "      <td>82.068035</td>\n",
       "      <td>21.652287</td>\n",
       "      <td>1.110788</td>\n",
       "      <td>0.806226</td>\n",
       "      <td>1.128280</td>\n",
       "      <td>1.11921</td>\n",
       "      <td>0.624264</td>\n",
       "      <td>1.167748</td>\n",
       "      <td>1.288057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>1.48324</td>\n",
       "      <td>1.046536</td>\n",
       "      <td>0.889087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.834523</td>\n",
       "      <td>1.068133</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.325987</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1991.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>836.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year  Body_Count  Length_Minutes  IMDB_Rating        Bob  \\\n",
       "count   421.000000  421.000000      421.000000   421.000000  16.000000   \n",
       "mean   1996.491686   53.287411      115.427553     6.882898   3.875000   \n",
       "std      10.913210   82.068035       21.652287     1.110788   0.806226   \n",
       "min    1949.000000    0.000000       79.000000     2.000000   3.000000   \n",
       "25%    1991.000000   11.000000      100.000000     6.200000   3.000000   \n",
       "50%    2000.000000   28.000000      111.000000     6.900000   4.000000   \n",
       "75%    2005.000000   61.000000      127.000000     7.700000   4.250000   \n",
       "max    2009.000000  836.000000      201.000000     9.300000   5.000000   \n",
       "\n",
       "          Arthur    George      Peter       John      Terry    ...     \\\n",
       "count  36.000000  20.00000  17.000000  11.000000  12.000000    ...      \n",
       "mean    3.388889   1.90000   3.470588   4.181818   3.750000    ...      \n",
       "std     1.128280   1.11921   0.624264   1.167748   1.288057    ...      \n",
       "min     1.000000   1.00000   2.000000   2.000000   1.000000    ...      \n",
       "25%     2.750000   1.00000   3.000000   4.000000   3.000000    ...      \n",
       "50%     3.000000   1.50000   4.000000   5.000000   4.000000    ...      \n",
       "75%     4.000000   3.00000   4.000000   5.000000   5.000000    ...      \n",
       "max     5.000000   5.00000   4.000000   5.000000   5.000000    ...      \n",
       "\n",
       "         Cedric    David  Nathaniel      Helen  Elaine      Toni     Joanne  \\\n",
       "count  5.000000  5.00000  15.000000  21.000000       1  8.000000  11.000000   \n",
       "mean   3.000000  2.80000   3.666667   4.095238       4  3.125000   3.409091   \n",
       "std    1.581139  1.48324   1.046536   0.889087     NaN  0.834523   1.068133   \n",
       "min    1.000000  1.00000   2.000000   2.000000       4  2.000000   1.000000   \n",
       "25%    2.000000  2.00000   3.000000   4.000000       4  3.000000   3.000000   \n",
       "50%    3.000000  3.00000   4.000000   4.000000       4  3.000000   3.500000   \n",
       "75%    4.000000  3.00000   4.500000   5.000000       4  3.000000   4.000000   \n",
       "max    5.000000  5.00000   5.000000   5.000000       4  5.000000   5.000000   \n",
       "\n",
       "       Elizabeth       Mary   Adeline  \n",
       "count   5.000000  14.000000  2.000000  \n",
       "mean    4.600000   2.714286  3.000000  \n",
       "std     0.547723   1.325987  1.414214  \n",
       "min     4.000000   1.000000  2.000000  \n",
       "25%     4.000000   2.000000  2.500000  \n",
       "50%     5.000000   2.000000  3.000000  \n",
       "75%     5.000000   3.750000  3.500000  \n",
       "max     5.000000   5.000000  4.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 2\n",
    "The movies data is now in a data frame which contains one column for each user rating the movie. Have a look at the data, what do the entries with 'NaN' in them mean?\n",
    "\n",
    "*5 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Question 2\n",
    "\n",
    "NaN stands for Not a Number. It means that there is either no value, sparse data, incorrect data, or some other error that causes the value to not appear to be a number.\n",
    "\n",
    "In the case of the data above, a user has only one data point, which is too sparse to take the standard deviation, which will show NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Data\n",
    "\n",
    "We will now prepare the data set for processing. To do this we are going to conver the data into a new format using the `melt` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_names = list(set(movies.columns)-set(movies.columns[:9]))\n",
    "Y = pd.melt(movies.reset_index(), id_vars=['Film', 'index'], \n",
    "            var_name='user', value_name='rating', \n",
    "            value_vars=user_names)\n",
    "Y = Y.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            index      rating\n",
      "count  374.000000  374.000000\n",
      "mean   226.291444    3.402406\n",
      "std    125.022714    1.136393\n",
      "min      2.000000    1.000000\n",
      "25%    107.000000    3.000000\n",
      "50%    238.500000    3.000000\n",
      "75%    342.750000    4.000000\n",
      "max    420.000000    5.000000\n",
      "                                                    Film  index       user  \\\n",
      "33                                          The Bank Job     33      Julie   \n",
      "36                                                Batman     36      Julie   \n",
      "40                                 Ying hung boon sik II     40      Julie   \n",
      "53                                  The Bourne Ultimatum     53      Julie   \n",
      "54                                            Braveheart     54      Julie   \n",
      "91                                       Wo hu cang long     91      Julie   \n",
      "166                                      Sat sau ji wong    166      Julie   \n",
      "178                                        Batman Begins    178      Julie   \n",
      "183                                             Iron Man    183      Julie   \n",
      "200                                              Kung fu    200      Julie   \n",
      "221    The Lord of the Rings: The Fellowship of the Ring    221      Julie   \n",
      "252                                            The Mummy    252      Julie   \n",
      "283                                         The Prestige    283      Julie   \n",
      "346                                            Star Wars    346      Julie   \n",
      "358       Sweeney Todd: The Demon Barber of Fleet Street    358      Julie   \n",
      "411                                X-Men: The Last Stand    411      Julie   \n",
      "524                                         The Departed    103       Tony   \n",
      "547                         4: Rise of the Silver Surfer    126       Tony   \n",
      "600    Indiana Jones and the Kingdom of the Crystal S...    179       Tony   \n",
      "601                                           Inside Man    180       Tony   \n",
      "618                                            King Kong    197       Tony   \n",
      "620                                  Kiss Kiss Bang Bang    199       Tony   \n",
      "629                                  The Legend of Zorro    208       Tony   \n",
      "655                                    The Mask of Zorro    234       Tony   \n",
      "681                                            Octopussy    260       Tony   \n",
      "697                             The Phantom of the Opera    276       Tony   \n",
      "704                                         The Prestige    283       Tony   \n",
      "759                                              Spartan    338       Tony   \n",
      "870                                            Atonement     28    Matilda   \n",
      "949                                             Die Hard    107    Matilda   \n",
      "...                                                  ...    ...        ...   \n",
      "11041                                    The Dark Knight     95  Nathaniel   \n",
      "11073                     Fear and Loathing in Las Vegas    127  Nathaniel   \n",
      "11109                                            Hellboy    163  Nathaniel   \n",
      "11135                                             Jumper    189  Nathaniel   \n",
      "11191                                Mission: Impossible    245  Nathaniel   \n",
      "11223  Pirates of the Caribbean: The Curse of the Bla...    277  Nathaniel   \n",
      "11237                                              Rambo    291  Nathaniel   \n",
      "11246                                     Reservoir Dogs    300  Nathaniel   \n",
      "11253                                            RoboCop    307  Nathaniel   \n",
      "11260                                    The Running Man    314  Nathaniel   \n",
      "11264                                           Scarface    318  Nathaniel   \n",
      "11278                                           Sin City    332  Nathaniel   \n",
      "11304     Sweeney Todd: The Demon Barber of Fleet Street    358  Nathaniel   \n",
      "11406                                 Ying hung boon sik     39      Peter   \n",
      "11431                                      Casino Royale     64      Peter   \n",
      "11450                                        Constantine     83      Peter   \n",
      "11462                                    The Dark Knight     95      Peter   \n",
      "11493                       4: Rise of the Silver Surfer    126      Peter   \n",
      "11543                                          In Bruges    176      Peter   \n",
      "11643                           The Phantom of the Opera    276      Peter   \n",
      "11644  Pirates of the Caribbean: The Curse of the Bla...    277      Peter   \n",
      "11645         Pirates of the Caribbean: Dead Man's Chest    278      Peter   \n",
      "11668                          Resident Evil: Apocalypse    301      Peter   \n",
      "11688                                      Secret Window    321      Peter   \n",
      "11697                           The Silence of the Lambs    330      Peter   \n",
      "11704                                            Snatch.    337      Peter   \n",
      "11709                                 Star Trek: Nemesis    342      Peter   \n",
      "11736                                There Will Be Blood    369      Peter   \n",
      "11747                                    The Transporter    380      Peter   \n",
      "11779                                                xXx    412      Peter   \n",
      "\n",
      "       rating  \n",
      "33          5  \n",
      "36          4  \n",
      "40          4  \n",
      "53          5  \n",
      "54          5  \n",
      "91          4  \n",
      "166         3  \n",
      "178         4  \n",
      "183         3  \n",
      "200         3  \n",
      "221         5  \n",
      "252         4  \n",
      "283         3  \n",
      "346         4  \n",
      "358         4  \n",
      "411         3  \n",
      "524         4  \n",
      "547         4  \n",
      "600         4  \n",
      "601         5  \n",
      "618         2  \n",
      "620         2  \n",
      "629         5  \n",
      "655         4  \n",
      "681         4  \n",
      "697         5  \n",
      "704         5  \n",
      "759         2  \n",
      "870         4  \n",
      "949         4  \n",
      "...       ...  \n",
      "11041       5  \n",
      "11073       5  \n",
      "11109       4  \n",
      "11135       2  \n",
      "11191       2  \n",
      "11223       3  \n",
      "11237       3  \n",
      "11246       5  \n",
      "11253       3  \n",
      "11260       4  \n",
      "11264       5  \n",
      "11278       4  \n",
      "11304       3  \n",
      "11406       3  \n",
      "11431       3  \n",
      "11450       4  \n",
      "11462       4  \n",
      "11493       2  \n",
      "11543       4  \n",
      "11643       4  \n",
      "11644       4  \n",
      "11645       3  \n",
      "11668       3  \n",
      "11688       4  \n",
      "11697       4  \n",
      "11704       4  \n",
      "11709       3  \n",
      "11736       4  \n",
      "11747       3  \n",
      "11779       3  \n",
      "\n",
      "[374 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print( Y.describe() )\n",
    "print( Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 3\n",
    "\n",
    "What is a pivot table? What does the `pandas` command `pd.melt` do? \n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We now need a measure for determining the similarity between the item and the user: how close the user is sitting to the item in the rooom if you like. We are going to use the inner product between the vector representing the item and the vector representing the user. \n",
    "\n",
    "An inner product (or [dot product](http://en.wikipedia.org/wiki/Dot_product)) between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is written as $\\mathbf{a}\\cdot\\mathbf{b}$. Or in vector notation we sometimes write it as $\\mathbf{a}^\\top\\mathbf{b}$. An inner product is simply the sume of the products of each element of the vector,\n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = \\sum_{i} a_i b_i\n",
    "$$\n",
    "The inner product can be seen as a measure of similarity. The inner product gives us the cosine of the angle between the two vectors multiplied by their length. The smaller the angle between two vectors the larger the inner product. \n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = |\\mathbf{a}||\\mathbf{b}| \\cos(\\theta)\n",
    "$$\n",
    "where $\\theta$ is the angle between two vectors and $|\\mathbf{a}|$ and $|\\mathbf{b}|$ are the respective lengths of the two vectors.\n",
    "\n",
    "Since we want each user to be sitting near each item, then we want the inner product to be large for any two items which are rated highly by that user. We can do this by trying to force the inner product $\\mathbf{u}_i^\\top\\mathbf{v}_j$ to be similar to the rating given by the user, $y_{i,j}$. To ensure this we will use a least squares objective function for all user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The error function (or objective function, or cost function) we will choose is known as 'sum of squares', we will aim to minimize the sum of squared squared error between the inner product of $\\mathbf{u}_i$ and $\\mathbf{v}_i$ and the observed score for the user/item pairing, given by $y_{i, j}$. \n",
    "\n",
    "The total objective function can be written as\n",
    "$$\n",
    "E(\\mathbf{U}, \\mathbf{V}) = \\sum_{i,j} s_{i,j} (y_{i,j} - \\mathbf{u}_i^\\top \\mathbf{v}_j)^2\n",
    "$$\n",
    "where $s_{i,j}$ is an *indicator* variable that is 1 if user $i$ has rated item $j$ and is zero otherwise. Here $\\mathbf{U}$ is the matrix made up of all the vectors $\\mathbf{u}$,\n",
    "$$\n",
    "\\mathbf{U} = \\begin{bmatrix} \\mathbf{u}_1 \\dots \\mathbf{u}_n\\end{bmatrix}^\\top\n",
    "$$\n",
    "where we note that $i$th *row* of $\\mathbf{U}$ contains the vector associated with the $i$th user and $n$ is the total number of users. This form of matrix is known as a *design matrix*. Similarly, we define the matrix\n",
    "$$\n",
    "\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_1 \\dots \\mathbf{v}_m\\end{bmatrix}^\\top\n",
    "$$\n",
    "where again the $j$th row of $\\mathbf{V}$ contains the vector associated with the $j$th item and $m$ is the total number o items in the data set.\n",
    "\n",
    "## Objective Optimization\n",
    "\n",
    "The idea is to mimimize this objective. A standard, simple, technique for minimizing an objective is *gradient descent* or *steepest descent*. In gradient descent we simply choose to update each parameter in the model by subtracting a multiple of the objective function's gradient with respect to the parameters. So for a parameter $u_{i,j}$ from the matrix $\\mathbf{U}$ we would have an update as follows:\n",
    "$$\n",
    "u_{i,j} \\leftarrow u_{i,j} - \\eta \\frac{\\text{d} E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{i,j}} \n",
    "$$\n",
    "where $\\eta$ (which is pronounced *eta* in English) is a Greek letter representing the *learning rate*.  \n",
    "\n",
    "We can compute the gradient of the objective function with respect to $u_{k,\\ell}$ as\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{k,\\ell}} = 2\\sum_{j} s_{k,j}v_{j,\\ell}(y_{k,\\ell} - \\mathbf{u}_k^\\top\\mathbf{v}_{j}). \n",
    "$$\n",
    "Similarly each parameter $v_{i,j}$ needs to be updated according to its gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 4\n",
    "\n",
    "What is the gradient of the objective function with respect to $v_{k, \\ell}$? Write your answer in the box below, and explain which differentiation techniques you used to get there. You will be expected to justify your answer in class by oral questioning. Create a function for computing this gradient that is used in the algorithm below.\n",
    "\n",
    "*20 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 4 Code Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent Algorithm\n",
    "\n",
    "In the steepest descent algorithm we aim to minimize the objective function by subtacting the gradient of the objective function from the parameters. To start with though, we need initial values for the matrix $\\mathbf{U}$ and the matrix $\\mathbf{V}$. Let's create them as `pandas` data frames and initialise them randomly with small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.01\n",
    "U = pd.DataFrame(np.random.normal(size=(len(user_names), q))*0.001, index=user_names)\n",
    "V = pd.DataFrame(np.random.normal(size=(len(movies.index), q))*0.001, index=movies.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will subtract the mean from the rating before we try and predict them predictions. Have a think about why this might be a good idea (hint, what will the gradients be if we don't subtract the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y['rating'] -= Y['rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the initial values set, we can start the optimization. First we define a function for the gradient of the objective and the objective function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective_gradient(Y, U, V):\n",
    "    gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "    gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "    obj = 0.\n",
    "    for ind, series in Y.iterrows():\n",
    "        film = series['index']\n",
    "        user = series['user']\n",
    "        rating = series['rating']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        diff = prediction - rating # vTu - y\n",
    "        obj += diff*diff\n",
    "        gU.loc[user] += 2*diff*V.loc[film]\n",
    "        gV.loc[film] += 2*diff*U.loc[user]\n",
    "    return obj, gU, gV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our simple optimisation route. This allows us to observe the objective function as the optimization proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  481.687811854\n",
      "Iteration 1 Objective function:  481.687740167\n",
      "Iteration 2 Objective function:  481.687663669\n",
      "Iteration 3 Objective function:  481.687578357\n",
      "Iteration 4 Objective function:  481.687479627\n",
      "Iteration 5 Objective function:  481.687361949\n",
      "Iteration 6 Objective function:  481.687218467\n",
      "Iteration 7 Objective function:  481.687040488\n",
      "Iteration 8 Objective function:  481.686816833\n",
      "Iteration 9 Objective function:  481.686532982\n",
      "Iteration 10 Objective function:  481.686169945\n",
      "Iteration 11 Objective function:  481.685702761\n",
      "Iteration 12 Objective function:  481.685098508\n",
      "Iteration 13 Objective function:  481.684313632\n",
      "Iteration 14 Objective function:  481.68329038\n",
      "Iteration 15 Objective function:  481.681952017\n",
      "Iteration 16 Objective function:  481.68019641\n",
      "Iteration 17 Objective function:  481.6778874\n",
      "Iteration 18 Objective function:  481.674843208\n",
      "Iteration 19 Objective function:  481.670820829\n",
      "Iteration 20 Objective function:  481.665495006\n",
      "Iteration 21 Objective function:  481.658429902\n",
      "Iteration 22 Objective function:  481.649040893\n",
      "Iteration 23 Objective function:  481.636543032\n",
      "Iteration 24 Objective function:  481.619881504\n",
      "Iteration 25 Objective function:  481.597637789\n",
      "Iteration 26 Objective function:  481.567903066\n",
      "Iteration 27 Objective function:  481.528107512\n",
      "Iteration 28 Objective function:  481.474790381\n",
      "Iteration 29 Objective function:  481.403290792\n",
      "Iteration 30 Objective function:  481.307332926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1e2d69038d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Objective function: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mU\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgU\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-0096e75510c6>\u001b[0m in \u001b[0;36mobjective_gradient\u001b[1;34m(Y, U, V)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrating\u001b[0m \u001b[1;31m# vTu - y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mgU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mgV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgV\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nathan/anaconda3/lib/python3.4/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# we are updating inplace so we want to ignore is_copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             self._update_inplace(result.reindex_like(self,copy=False)._data,\n\u001b[1;32m--> 187\u001b[1;33m                                  verify_is_copy=False)\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nathan/anaconda3/lib/python3.4/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_update_inplace\u001b[1;34m(self, result, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;31m# we want to call the generic version and not the IndexOpsMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;31m# ndarray compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nathan/anaconda3/lib/python3.4/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_update_inplace\u001b[1;34m(self, result, verify_is_copy)\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1641\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_is_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nathan/anaconda3/lib/python3.4/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[1;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \"\"\"\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m         \u001b[0mcacher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cacher'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[0mref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcacher\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/nathan/anaconda3/lib/python3.4/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2135\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2136\u001b[0m         \"\"\"After regular attribute access, try looking up the name\n\u001b[0;32m   2137\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mallows\u001b[0m \u001b[0msimpler\u001b[0m \u001b[0maccess\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minteractive\u001b[0m \u001b[0muse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "iterations = 100\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_gradient(Y, U, V)\n",
    "    print(\"Iteration\", i, \"Objective function: \", obj)\n",
    "    U -= learn_rate*gU\n",
    "    V -= learn_rate*gV\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 5\n",
    "\n",
    "What happens as you increase the number of iterations? What happens if you increase the learning rate?\n",
    "\n",
    "*10 marks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 Answer\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 5 Code Answer\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent or Robbins Monroe Algorithm\n",
    "\n",
    "Stochastic gradient descent involves updating separating each gradient update according to each separate observation, rather than summing over them all. It is an approximate optimization method, but it has proven convergence under certain conditions and can be much faster in practice. It is used widely by internet companies for doing machine learning in practice. For example, Facebook's ad ranking algorithm uses stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 6\n",
    "\n",
    "Create a stochastic gradient descent version of the algorithm. Monitor the objective function after every 1000 updates to ensure that it is decreasing. When you have finished, plot the movie map and the user map in two dimensions. Label the plots with the name of the movie or user.\n",
    "\n",
    "*30 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 6 Code Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Predictions can be made from the model of the appropriate rating for a given user, $i$, for a given film, $j$, by simply taking the inner product between their vectors $\\mathbf{u}_i$ and $\\mathbf{v}_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Our Map Enough? Are Our Data Enough?\n",
    "\n",
    "Is two dimensions really enough to capture the complexity of humans and their artforms? Perhaps we need even more dimensions to capture that complexity. Extending our books analogy further, consider how we should place books that have a historical timeframe as well as some geographical location. Do we really want books from the 2nd World War to sit alongside books from the Roman Empire? Books on the American invasion of Sicily in 1943 are perhaps less related to books about Carthage than those that study the Jewish Revolt from 66-70 (in the Roman Province of Judaea). So books that relate to subjects which are closer in time should be stored together. However, a student of rebellion against empire may also be interested in the relationship between the Jewish Revolt of 66-70 and the Indian Rebellion of 1857, nearly 1800 years later. Whilst the technologies are different, the psychology of the people is shared: a rebellious nation angainst their imperial masters, triggered by misrule with a religious and cultural background. To capture such complexities we would need further dimensions in our latent representation. But are further dimensions justified by the amount of data we have? Can we really understand the facets of a film that only has at most three or four ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "If you want to take this model further then you'll need more data. One possible source of data is the [`movielens` data set](http://grouplens.org/datasets/movielens/). They have data sets containing up to ten million movie ratings. The few ratings we were able to collect in the class are not enough to capture the rich structure underlying these films. Imagine if we assume that the ratings are uniformly distributed between 1 and 5. If you know something about information theory then you could use that to work out the maximum number of *bits* of information we could gain per rating. \n",
    "\n",
    "Now we'll download the movielens 100k data and see if we can extract information about these movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring resource: movielens100k\n",
      "\n",
      "Details of data: \n",
      "MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.\n",
      "\n",
      "This data set consists of:\n",
      "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
      "* Each user has rated at least 20 movies. \n",
      "        * Simple demographic info for the users (age, gender, occupation, zip)\n",
      "\n",
      "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set.\n",
      "\n",
      "Please cite:\n",
      "Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic Framework for Performing Collaborative Filtering. Proceedings of the 1999 Conference on Research and Development in Information Retrieval. Aug. 1999.\n",
      "\n",
      "After downloading the data will take up 536272 bytes of space.\n",
      "\n",
      "Data will be stored in /home/nathan/ods_data_cache/movielens100k.\n",
      "\n",
      "You must also agree to the following license:\n",
      "Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set.  The data set may be used for any research purposes under the following conditions:\n",
      "\n",
      "     * The user may not state or imply any endorsement from the\n",
      "       University of Minnesota or the GroupLens Research Group.\n",
      "\n",
      "     * The user must acknowledge the use of the data set in\n",
      "       publications resulting from the use of the data set, and must\n",
      "       send us an electronic or paper copy of those publications.\n",
      "\n",
      "     * The user may not redistribute the data without separate\n",
      "       permission.\n",
      "\n",
      "     * The user may not use this information for any commercial or\n",
      "       revenue-bearing purposes without first obtaining permission\n",
      "       from a faculty member of the GroupLens Research Project at the\n",
      "       University of Minnesota.\n",
      "\n",
      "If you have any further questions or comments, please contact GroupLens <grouplens-info@cs.umn.edu>.\n",
      "\n",
      "Do you wish to proceed with the download? [yes/no]\n",
      "yes\n",
      "Downloading  http://files.grouplens.org/datasets/movielens/ml-100k.zip -> /home/nathan/ods_data_cache/movielens100k/ml-100k.zip\n",
      "[==============================]   4.696/4.696MB\n",
      "Downloading  http://files.grouplens.org/datasets/movielens/ml-100k-README.txt -> /home/nathan/ods_data_cache/movielens100k/ml-100k-README.txt\n",
      "[==============================]   0.006/0.006MB\n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "d = pods.datasets.movielens100k()\n",
    "Y=d['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Question 7\n",
    "\n",
    "Use stochastic gradient descent to make a movie map for the movielens data. Plot the map of the movies when you are finished.\n",
    "\n",
    "*15 marks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for question 7 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
